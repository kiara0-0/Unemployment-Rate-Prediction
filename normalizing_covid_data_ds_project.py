# -*- coding: utf-8 -*-
"""Normalizing_Covid_Data_DS_Project.ipynb

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/1LQt3kDGu8oV6Z2azRrpT19GXPajRCdSm
"""

import pandas as pd

from google.colab import drive
drive.mount('/content/drive')

df1=pd.read_csv('/content/drive/MyDrive/CPIAUCSL.csv')
df2=pd.read_csv('/content/drive/MyDrive/FEDFUNDS.csv')
df3=pd.read_csv('/content/drive/MyDrive/GDP.csv')
df4=pd.read_csv('/content/drive/MyDrive/INDPRO.csv')
df5=pd.read_csv('/content/drive/MyDrive/JTSJOL.csv')
df6=pd.read_csv('/content/drive/MyDrive/UMCSENT.csv')
df7=pd.read_csv('/content/drive/MyDrive/USGOVT.csv')
df8=pd.read_csv('/content/drive/MyDrive/LFWA64TTUSM647S.csv')

df9=pd.read_csv('/content/drive/MyDrive/UNRATE.csv')

merged_df1 = pd.merge(df1, df2, on='DATE',how='left')
merged_df2 = pd.merge(df3, df4, on='DATE',how='right')
merged_df3 = pd.merge(df5, df6, on='DATE',how='left')
merged_df4 = pd.merge(df7, df8, on='DATE',how='left')
# merged_df5 = pd.merge(df9, df10, on='DATE',how='inner')

merged_df12 = pd.merge(merged_df1, merged_df2, on='DATE',how='left')
merged_df34 = pd.merge(merged_df3, merged_df4, on='DATE',how='left')

merged_df1234=pd.merge(merged_df12, merged_df34, on='DATE',how='left')

merged_df= pd.merge(merged_df1234, df9, on='DATE',how='left')

merged_df

merged_df=merged_df.rename(columns={"LFWA64TTUSM647S": "WAP", "CPIAUCSL": "CPI"})
merged_df.info()

merged_df.describe()

# threshold='2015-01-01'
# df=merged_df.drop(merged_df['DATE']<threshold)

merged_df['DATE'] = pd.to_datetime(merged_df['DATE'])

merged_df.info()

threshold_date = pd.to_datetime('2013-01-01')

df = merged_df[merged_df['DATE'] >= threshold_date]

df['UMCSENT'] = pd.to_numeric(df['UMCSENT'], errors='coerce')

df['GDP']=df['GDP'].ffill()

df

df['WAP']=df['WAP'].ffill()
df['JTSJOL']=df['JTSJOL'].ffill()
df['UMCSENT']=df['UMCSENT'].ffill()
df['USGOVT']=df['USGOVT'].ffill()

df['UNRATE_lag1'] = df['UNRATE'].shift(1)
df['CPI_lag1'] = df['CPI'].shift(1)
df['FEDFUNDS_lag1'] = df['FEDFUNDS'].shift(1)
df['GDP_lag1'] = df['GDP'].shift(1)
df['INDPRO_lag1'] = df['INDPRO'].shift(1)
df['JTSJOL_lag1'] = df['JTSJOL'].shift(1)
df['UMCSENT_lag1'] = df['UMCSENT'].shift(1)
df['USGOVT_lag1'] = df['USGOVT'].shift(1)
df['WAP_lag1'] = df['WAP'].shift(1)

df

"""##MAKING A DECISION TREE REGRESSOR BASED ON IF PANDEMIC OR NOT"""

df['pandemic_yes'] = 0
df.loc[(df['DATE'].dt.year >= 2020) & (df['DATE'].dt.year <= 2022), 'pandemic_yes'] = 1

# df = df.set_index("DATE")
df['FEDFUNDS_yoy'] = df['FEDFUNDS'].diff(periods=12) * 100
df['GDP_yoy'] = df['GDP'].pct_change(periods=12) * 100
df['CPI_yoy'] = df['CPI'].pct_change(periods=12) * 100
df['UMCSENT_yoy'] = df['UMCSENT'].pct_change(periods=12) * 100
df['INDPRO_yoy'] = df['INDPRO'].pct_change(periods=12) * 100
df['JTSJOL_yoy'] = df['JTSJOL'].pct_change(periods=12) * 100
df['USGOVT_yoy'] = df['USGOVT'].pct_change(periods=12) * 100
df['WAP_yoy'] = df['WAP'].pct_change(periods=12) * 100

df['GDP_lag1_yoy'] = df['GDP_lag1'].pct_change(periods=12) * 100
df['CPI_lag1_yoy'] = df['CPI_lag1'].pct_change(periods=12) * 100
df['UMCSENT_lag1_yoy'] = df['UMCSENT_lag1'].pct_change(periods=12) * 100
df['INDPRO_lag1_yoy'] = df['INDPRO_lag1'].pct_change(periods=12) * 100
df['JTSJOL_lag1_yoy'] = df['JTSJOL_lag1'].pct_change(periods=12) * 100
df['USGOVT_lag1_yoy'] = df['USGOVT_lag1'].pct_change(periods=12) * 100
df['WAP_lag1_yoy'] = df['WAP_lag1'].pct_change(periods=12) * 100

# Calculate 3-month annualized growth rate and update column names in one line each
df['FEDFUNDS_3m'] = df['FEDFUNDS_lag1'].diff(periods=3) * 100
df['GDP_3m_annualized'] = df['GDP_lag1'].pct_change(periods=3) * 100 * 4
df['CPI_3m_annualized'] = df['CPI_lag1'].pct_change(periods=3) * 100 * 4
df['UMCSENT_3m_annualized'] = df['UMCSENT_lag1'].pct_change(periods=3) * 100 * 4
df['INDPRO_3m_annualized'] = df['INDPRO_lag1'].pct_change(periods=3) * 100 * 4
df['JTSJOL_3m_annualized'] = df['JTSJOL_lag1'].pct_change(periods=3) * 100 * 4
df['USGOVT_3m_annualized'] = df['USGOVT_lag1'].pct_change(periods=3) * 100 * 4
df['WAP_3m_annualized'] = df['WAP_lag1'].pct_change(periods=3) * 100 * 4

# df["Pandemic_Yes"]=df[df["Date"]]

df = df.iloc[1:]
df = df.iloc[12:]

df.tail(30)

from sklearn.tree import DecisionTreeRegressor
from sklearn.model_selection import train_test_split
from sklearn.tree import plot_tree
import matplotlib.pyplot as plt
from sklearn.metrics import mean_absolute_percentage_error


features = ['pandemic_yes','GDP_lag1_yoy', 'CPI_lag1_yoy', 'UMCSENT_lag1_yoy', 'INDPRO_lag1_yoy', 'JTSJOL_lag1_yoy', 'USGOVT_lag1_yoy', 'WAP_lag1_yoy',
            'GDP_3m_annualized', 'CPI_3m_annualized', 'UMCSENT_3m_annualized', 'INDPRO_3m_annualized',
            'JTSJOL_3m_annualized', 'USGOVT_3m_annualized', 'WAP_3m_annualized']

# regressor = DecisionTreeRegressor(random_state=0)

X=df[features]
y=df['UNRATE']

X_train,X_test,y_train,y_test=train_test_split(X,y,test_size=0.2,random_state=42)

dt_model = DecisionTreeRegressor(random_state=1, min_samples_split=5, max_depth=3)
dt_model.fit(X_train, y_train)

y_pred = dt_model.predict(X_test)

# Visualize the decision tree
plt.figure(figsize=(20,10))  # Adjust the figure size as needed
plot_tree(dt_model, feature_names=features, filled=True)
plt.show()

print("MAPE: ",mean_absolute_percentage_error(y_test, y_pred))

# train_df = df[(df['DATE'] < '2020-01-01') | (df['DATE'] > '2021-12-31') & (df['DATE'] < '2024-01-01')]
# test_df = df[df['DATE'] >= '2024-01-01']

# # train_df.tail(30)

rows_with_nan = df[df['FEDFUNDS'].isna()].index
rows_with_nan

from sklearn.impute import SimpleImputer

# Create an imputer object and specify the imputation strategy
imputer = SimpleImputer(strategy='mean')
df['UNRATE_lag1'] = imputer.fit_transform(df[['UNRATE_lag1']])

# train_df = train_df.set_index("DATE")
train_df=df

# train_df.isnull().values.any()

null_values = train_df.isnull().any()

# Display columns with null values (if any)a
columns_with_null = null_values[null_values].index.tolist()
print("Columns with null values:", columns_with_null)

# Check if there are any null values in the entire DataFrame
if null_values.any():
    print("There are null values in the DataFrame.")
else:
    print("There are no null values in the DataFrame.")

train_df = train_df[(train_df['DATE'] < '2020-01-01') | (train_df['DATE'] > '2021-12-31')]
test_df = train_df[train_df['DATE'] >= '2024-03-01']
train_df.set_index("DATE", inplace = True)
train_df.head(15)

import matplotlib.pyplot as plt

# Plot the required columns
plt.figure(figsize=(14, 7))

# Assuming 'UNEM' refers to unemployment rate
plt.plot(train_df.index, train_df['UNRATE'], label='Unemployment Rate', color='blue')

# Assuming 'CPI_yoy' refers to the year-over-year change in the Consumer Price Index
plt.plot(train_df.index, train_df['CPI_yoy'], label='CPI Year-Over-Year Change', color='green')

# Plotting 'FEDFUNDS'
plt.plot(train_df.index, train_df['FEDFUNDS'], label='Federal Funds Rate', color='red',)

# Adding labels and title
plt.xlabel('Date')
plt.ylabel('Percentage')
plt.title('Economic Indicators Over Time')
plt.legend()

# Show the plot
plt.show()

# plt.figure(figsize=(14, 7))

# # Assuming 'UNEM' refers to unemployment rate
# plt.plot(train_df[(train_df["DATE"]<"2015-01-01") & (train_df["DATE"]>"2017-01-01")].index, train_df[(train_df["DATE"]<"2017-01-01") & (train_df["DATE"]>"2015-01-01")]['UNRATE'], label='Unemployment Rate', color='blue')

# import numpy as np
# print(X_train.isnull().sum())  # Check for null values
# print(X_train.isin([np.nan, np.inf, -np.inf]).sum())  # Check for inf and NaN values

"""**OLS: YEAR Over YEAR**"""

from sklearn.linear_model import LinearRegression
from sklearn.model_selection import train_test_split
import statsmodels.api as sm
from sklearn.metrics import mean_squared_error

columns_to_include = ['GDP_lag1_yoy', 'CPI_lag1_yoy', 'UMCSENT_lag1_yoy', 'INDPRO_lag1_yoy','JTSJOL_lag1_yoy','USGOVT_lag1_yoy','WAP_lag1_yoy']
# columns_to_include=['GDP_3m_annualized','FEDFUNDS_3m','CPI_3m_annualized','UMCSENT_3m_annualized','INDPRO_3m_annualized','JTSJOL_3m_annualized','USGOVT_3m_annualized','WAP_3m_annualized']

# Preparing the data
X = train_df[columns_to_include]  # Exclude target variable and date
y = train_df['UNRATE']
X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, shuffle=False)

# Add a constant to the model (the intercept)
X_train_sm = sm.add_constant(X_train)

train_df.head(5)
# X_train.head(10)

# Fit the model
model = sm.OLS(y_train, X_train_sm)
results = model.fit()

# Print the summary of the model
print(results.summary())

# Predict and evaluate (adding constant to the test set)
X_test_sm = sm.add_constant(X_test)
predictions = results.predict(X_test_sm)

# Calculate MSE manually or use a function from sklearn.metrics if desired
mse = ((predictions - y_test) ** 2).mean()
print(f'MSE: {mse}')

print("MAPE: ",mean_absolute_percentage_error(y_test, predictions))
# print("MSE: ",mean_squared_error(y_test, predictions))

import matplotlib.pyplot as plt

# Assuming your predictions and y_test are already defined as before

# Plotting actual vs. predicted values
plt.figure(figsize=(14, 7))
plt.plot(y_test.index, y_test, label='Actual', marker='o', color='blue')
plt.plot(y_test.index, predictions, label='Predicted', marker='x', color='red')
plt.title('Actual vs. Predicted Values')
plt.xlabel('Index')
plt.ylabel('UNRATE')
plt.legend()
plt.show()

# Plotting residuals
residuals = y_test - predictions
plt.figure(figsize=(14, 7))
plt.plot(y_test.index, residuals, label='Residuals', marker='o', color='green')
plt.axhline(y=0, color='grey', linestyle='--')
plt.title('Residuals over Time')
plt.xlabel('Index')
plt.ylabel('Residual')
plt.legend()
plt.show()

"""OLS: 3Month Annualized"""

X_train_sm

from sklearn.linear_model import LinearRegression
from sklearn.model_selection import train_test_split
import statsmodels.api as sm
from sklearn.metrics import mean_squared_error

columns_to_include = ['GDP_3m_annualized', 'CPI_3m_annualized', 'UMCSENT_3m_annualized', 'INDPRO_3m_annualized','JTSJOL_3m_annualized','USGOVT_3m_annualized','WAP_3m_annualized']

# Preparing the data
X = train_df[columns_to_include]  # Exclude target variable and date
y = train_df['UNRATE']
X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, shuffle=False)

# Add a constant to the model (the intercept)
X_train_sm = sm.add_constant(X_train)

# Fit the model
model = sm.OLS(y_train, X_train_sm)
results = model.fit()

# Print the summary of the model
print(results.summary())

# Predict and evaluate (adding constant to the test set)
X_test_sm = sm.add_constant(X_test)
predictions = results.predict(X_test_sm)

# Calculate MSE manually or use a function from sklearn.metrics if desired
mse = ((predictions - y_test) ** 2).mean()
print(f'MSE: {mse}')

print("MAPE: ",mean_absolute_percentage_error(y_test, predictions))

import matplotlib.pyplot as plt

# Assuming your predictions and y_test are already defined as before

# Plotting actual vs. predicted values
plt.figure(figsize=(14, 7))
plt.plot(y_test.index, y_test, label='Actual', marker='o', color='blue')
plt.plot(y_test.index, predictions, label='Predicted', marker='x', color='red')
plt.title('Actual vs. Predicted Values')
plt.xlabel('Index')
plt.ylabel('UNRATE')
plt.legend()
plt.show()

# Plotting residuals
residuals = y_test - predictions
plt.figure(figsize=(14, 7))
plt.plot(y_test.index, residuals, label='Residuals', marker='o', color='green')
plt.axhline(y=0, color='grey', linestyle='--')
plt.title('Residuals over Time')
plt.xlabel('Index')
plt.ylabel('Residual')
plt.legend()
plt.show()

"""##SARIMAX

"""

from statsmodels.tsa.statespace.sarimax import SARIMAX

exog = train_df[['GDP_3m_annualized', 'CPI_3m_annualized', 'UMCSENT_3m_annualized', 'INDPRO_3m_annualized','JTSJOL_3m_annualized','USGOVT_3m_annualized','WAP_3m_annualized']]

# Define the target variable
endog = train_df['UNRATE']

# Fit an ARIMAX model - starting with a simple ARIMA(1,0,1) model
model = SARIMAX(endog, exog=exog, order=(1,1,1), seasonal_order=(0,0,0,12), trend='c', enforce_stationarity=False, enforce_invertibility=False)
results = model.fit()

# Summary of the model
results_summary = results.summary()
results_summary

from statsmodels.tsa.statespace.sarimax import SARIMAX

exog = train_df[['GDP_lag1_yoy', 'CPI_lag1_yoy', 'UMCSENT_lag1_yoy', 'INDPRO_lag1_yoy','JTSJOL_lag1_yoy','USGOVT_lag1_yoy','WAP_lag1_yoy']]

# Define the target variable
endog = train_df['UNRATE']

# Fit an ARIMAX model - starting with a simple ARIMA(1,0,1) model
model = SARIMAX(endog, exog=exog, order=(1,0,1), seasonal_order=(0,0,0,12), trend='c', enforce_stationarity=False, enforce_invertibility=False)
results = model.fit()

# Summary of the model
results_summary = results.summary()
results_summary

"""###LAGGED Exog Variables YoY###"""

# Calculate the split index for the last 20% of the data
split_idx = int(len(train_df) * 0.8)

# Define training and testing data
train_endog = train_df['UNRATE'][:split_idx]
test_endog = train_df['UNRATE'][split_idx:]
train_exog = train_df[['GDP_lag1_yoy', 'CPI_lag1_yoy', 'UMCSENT_lag1_yoy', 'INDPRO_lag1_yoy','JTSJOL_lag1_yoy','USGOVT_lag1_yoy','WAP_lag1_yoy']][:split_idx]
test_exog = train_df[['GDP_lag1_yoy', 'CPI_lag1_yoy', 'UMCSENT_lag1_yoy', 'INDPRO_lag1_yoy','JTSJOL_lag1_yoy','USGOVT_lag1_yoy','WAP_lag1_yoy']][split_idx:]

# Fit the ARIMAX model on the training set
model_train = SARIMAX(train_endog, exog=train_exog, order=(1,0,1), seasonal_order=(0,0,0,12), trend='c', enforce_stationarity=False, enforce_invertibility=False)
results_train = model_train.fit()

# Forecast over the test set duration
forecasts = results_train.get_forecast(steps=len(test_endog), exog=test_exog)
forecast_means = forecasts.predicted_mean
forecast_intervals = forecasts.conf_int()

# Create plot
plt.figure(figsize=(10, 6))
plt.plot(train_df.index[:split_idx], train_endog, label='Training Data')
plt.plot(train_df.index[split_idx:], test_endog, label='Actual Test Data')
plt.plot(train_df.index[split_idx:], forecast_means, label='Forecasted Data', color='red')
plt.fill_between(train_df.index[split_idx:], forecast_intervals.iloc[:, 0], forecast_intervals.iloc[:, 1], color='pink', alpha=0.3)
plt.title('Forecast vs Actuals')
plt.xlabel('Date')
plt.ylabel('UNRATE')
plt.legend()
plt.show()

print("MAPE: ",mean_absolute_percentage_error(test_endog, forecasts.predicted_mean))
print("MSE: ",mean_squared_error(test_endog, forecasts.predicted_mean))

# Calculate the split index for the last 20% of the data
split_idx = int(len(train_df) * 0.8)

# Define training and testing data
train_endog = train_df['UNRATE'][:split_idx]
test_endog = train_df['UNRATE'][split_idx:]
train_exog = train_df[['GDP_3m_annualized', 'CPI_3m_annualized', 'UMCSENT_3m_annualized', 'INDPRO_3m_annualized', 'JTSJOL_3m_annualized', 'USGOVT_3m_annualized', 'WAP_3m_annualized']][:split_idx]
test_exog = train_df[['GDP_3m_annualized', 'CPI_3m_annualized', 'UMCSENT_3m_annualized', 'INDPRO_3m_annualized', 'JTSJOL_3m_annualized', 'USGOVT_3m_annualized', 'WAP_3m_annualized']][split_idx:]

# Fit the ARIMAX model on the training set
model_train = SARIMAX(train_endog, exog=train_exog, order=(1,0,1), seasonal_order=(0,0,0,12), trend='c', enforce_stationarity=False, enforce_invertibility=False)
results_train = model_train.fit()

# Forecast over the test set duration
forecasts = results_train.get_forecast(steps=len(test_endog), exog=test_exog)
forecast_means = forecasts.predicted_mean
forecast_intervals = forecasts.conf_int()

# Create plot
plt.figure(figsize=(10, 6))
plt.plot(train_df.index[:split_idx], train_endog, label='Training Data')
plt.plot(train_df.index[split_idx:], test_endog, label='Actual Test Data')
plt.plot(train_df.index[split_idx:], forecast_means, label='Forecasted Data', color='red')
plt.fill_between(train_df.index[split_idx:], forecast_intervals.iloc[:, 0], forecast_intervals.iloc[:, 1], color='pink', alpha=0.3)
plt.title('Forecast vs Actuals')
plt.xlabel('Date')
plt.ylabel('UNRATE')
plt.legend()
plt.show()


print("MAPE: ",mean_absolute_percentage_error(test_endog, forecasts.predicted_mean))
print("MSE: ",mean_squared_error(test_endog, forecasts.predicted_mean))

"""##ARIMAX

"""

from pandas.plotting import autocorrelation_plot

series=df["UNRATE"]
autocorrelation_plot(series)

from statsmodels.tsa.arima.model import ARIMA
from sklearn.metrics import mean_absolute_percentage_error, mean_squared_error

# Calculate the split index for the last 20% of the data
split_idx = int(len(train_df) * 0.8)

# Define training and testing data
train_endog = train_df['UNRATE'][:split_idx]
test_endog = train_df['UNRATE'][split_idx:]
train_exog = train_df[['GDP_3m_annualized', 'CPI_3m_annualized', 'UMCSENT_3m_annualized', 'INDPRO_3m_annualized', 'JTSJOL_3m_annualized', 'USGOVT_3m_annualized', 'WAP_3m_annualized']][:split_idx]
test_exog = train_df[['GDP_3m_annualized', 'CPI_3m_annualized', 'UMCSENT_3m_annualized', 'INDPRO_3m_annualized', 'JTSJOL_3m_annualized', 'USGOVT_3m_annualized', 'WAP_3m_annualized']][split_idx:]

# Fit the ARIMAX model on the training set
model_train = ARIMA(train_endog, exog=train_exog, order=(1,0,1), trend='c', enforce_stationarity=False, enforce_invertibility=False)
results_train = model_train.fit()

# Forecast over the test set duration
forecasts = results_train.get_forecast(steps=len(test_endog), exog=test_exog)
forecast_means = forecasts.predicted_mean
forecast_intervals = forecasts.conf_int()

# Create plot
plt.figure(figsize=(10, 6))
plt.plot(train_df.index[:split_idx], train_endog, label='Training Data')
plt.plot(train_df.index[split_idx:], test_endog, label='Actual Test Data')
plt.plot(train_df.index[split_idx:], forecast_means, label='Forecasted Data', color='red')
plt.fill_between(train_df.index[split_idx:], forecast_intervals.iloc[:, 0], forecast_intervals.iloc[:, 1], color='pink', alpha=0.3)
plt.title('Forecast vs Actuals')
plt.xlabel('Date')
plt.ylabel('UNRATE')
plt.legend()
plt.show()


print("MAPE: ",mean_absolute_percentage_error(test_endog, forecasts.predicted_mean))
print("MSE: ",mean_squared_error(test_endog, forecasts.predicted_mean))

"""# **Decision Tree Model**"""

train_df['UNRATE_direction'] = (train_df['UNRATE'] > train_df['UNRATE'].shift(1)).astype(int)

features = ['GDP_lag1_yoy', 'CPI_lag1_yoy', 'UMCSENT_lag1_yoy', 'INDPRO_lag1_yoy', 'JTSJOL_lag1_yoy', 'USGOVT_lag1_yoy', 'WAP_lag1_yoy',
            'GDP_3m_annualized', 'CPI_3m_annualized', 'UMCSENT_3m_annualized', 'INDPRO_3m_annualized',
            'JTSJOL_3m_annualized', 'USGOVT_3m_annualized', 'WAP_3m_annualized']

from sklearn.model_selection import train_test_split

X = train_df[features]
y = train_df['UNRATE_direction']

X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42, shuffle=False)

# from sklearn.model_selection import train_test_split

# X = train_df[features]
# y = train_df['UNRATE_direction']

# X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)

X_train = X_train.dropna()
y_train = y_train[X_train.index]

X_test = X_test.dropna()
y_test = y_test[X_test.index]

from sklearn.tree import DecisionTreeClassifier
import pandas as pd
from sklearn.tree import export_graphviz
import pydotplus
from IPython.display import Image
from sklearn.metrics import accuracy_score, confusion_matrix

dt_model = DecisionTreeClassifier(random_state=1, max_depth=3, min_samples_split=2, class_weight={0:0.1,1:0.9})
dt_model.fit(X_train, y_train)

y_pred = dt_model.predict(X_test)
accuracy = accuracy_score(y_test, y_pred)

confusion_mat = confusion_matrix(y_test, y_pred)

confusion_mat_df = pd.DataFrame(confusion_mat,
                                index=['Actual Decrease', 'Actual Increase'],
                                columns=['Predicted Decrease', 'Predicted Increase'])

print(f"Accuracy: {accuracy:.2f}")
print("Confusion Matrix:")
print(confusion_mat_df)

dot_data = export_graphviz(dt_model, out_file=None,
                           feature_names=features,
                           class_names=['Down', 'Up'],
                           filled=True, rounded=True,
                           special_characters=True)

graph = pydotplus.graph_from_dot_data(dot_data)

img = Image(graph.create_png())
display(img)

import seaborn as sns
sns.heatmap(confusion_mat, annot=True)

y_train.mean()

"""# **Logistic Regression Model**"""

from sklearn.linear_model import LogisticRegression
from sklearn.metrics import accuracy_score, confusion_matrix, precision_score, recall_score, f1_score
from sklearn.preprocessing import StandardScaler

scaler = StandardScaler()
X_train_scaled = scaler.fit_transform(X_train)
X_test_scaled = scaler.transform(X_test)

lr_model = LogisticRegression(random_state=42, max_iter=1000, solver='liblinear')
lr_model.fit(X_train_scaled, y_train)

lr_y_pred = lr_model.predict(X_test_scaled)
lr_accuracy = accuracy_score(y_test, lr_y_pred)
lr_confusion_mat = confusion_matrix(y_test, lr_y_pred)
lr_precision = precision_score(y_test, lr_y_pred)
lr_recall = recall_score(y_test, lr_y_pred)
lr_f1 = f1_score(y_test, lr_y_pred)

print("Logistic Regression Model:")
print(f"Accuracy: {lr_accuracy:.2f}")
print("Confusion Matrix:")
lr_cm_df = pd.DataFrame(lr_confusion_mat, index=['Actual Down', 'Actual Up'], columns=['Predicted Down', 'Predicted Up'])
print(lr_cm_df)
print(f"Precision: {lr_precision:.2f}")
print(f"Recall: {lr_recall:.2f}")
print(f"F1-score: {lr_f1:.2f}")

import seaborn as sns
sns.heatmap(lr_confusion_mat, annot=True)

"""##RANDOM FOREST CLASSIFIER"""

features = ['GDP_lag1_yoy', 'CPI_lag1_yoy', 'UMCSENT_lag1_yoy', 'INDPRO_lag1_yoy', 'JTSJOL_lag1_yoy', 'USGOVT_lag1_yoy', 'WAP_lag1_yoy',
            'GDP_3m_annualized', 'CPI_3m_annualized', 'UMCSENT_3m_annualized', 'INDPRO_3m_annualized',
            'JTSJOL_3m_annualized', 'USGOVT_3m_annualized', 'WAP_3m_annualized']

X = train_df[features]
y = train_df['UNRATE_direction']

X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42, shuffle=False)

# rfc_y_pred_proba=rfc_model.predict_proba(X_test_scaled)[:,1]
# threshold=0.4
# rfc_y_pred_th=np.where(rfc_y_pred_proba>=threshold,1,0)
# sum(rfc_y_pred_th),sum(rfc_y_pred)

from sklearn.ensemble import RandomForestClassifier

rfc_model=RandomForestClassifier(max_depth=3,random_state=42)
rfc_model.fit(X_train_scaled,y_train)

rfc_y_pred = rfc_model.predict(X_test_scaled)
rfc_y_pred_proba=rfc_model.predict_proba(X_test_scaled)[:,1]
threshold=0.45
rfc_y_pred_th=np.where(rfc_y_pred_proba>=threshold,1,0)

rfc_accuracy = accuracy_score(y_test, rfc_y_pred_th)
rfc_confusion_mat = confusion_matrix(y_test, rfc_y_pred_th)
rfc_precision = precision_score(y_test, rfc_y_pred_th)
rfc_recall = recall_score(y_test, rfc_y_pred_th)
rfc_f1 = f1_score(y_test, rfc_y_pred_th)

print("Random Forest Classifier:")
print(f"Accuracy: {rfc_accuracy:.2f}")
print("Confusion Matrix:")
rfc_cm_df = pd.DataFrame(rfc_confusion_mat, index=['Actual Decrease', 'Actual Increase'], columns=['Predicted Decrease', 'Predicted Increase'])
print(rfc_cm_df)
print(f"Precision: {rfc_precision:.2f}")
print(f"Recall: {rfc_recall:.2f}")
print(f"F1-score: {rfc_f1:.2f}")

sns.heatmap(rfc_confusion_mat, annot=True)

"""## **Neural Networks as a CLassification Algorithm**"""

from sklearn.preprocessing import StandardScaler
from sklearn.metrics import accuracy_score, confusion_matrix, precision_score, recall_score, f1_score
from tensorflow.keras.models import Sequential
from tensorflow.keras.layers import Dense

# Scale the input features
scaler = StandardScaler()
X_train_scaled = scaler.fit_transform(X_train)
X_test_scaled = scaler.transform(X_test)


model = Sequential()
model.add(Dense(64, activation='relu', input_shape=(X_train_scaled.shape[1],)))
model.add(Dense(32, activation='relu'))
model.add(Dense(1, activation='sigmoid'))


model.compile(optimizer='SGD', loss='binary_crossentropy', metrics=['accuracy'])


model.fit(X_train_scaled, y_train, epochs=50, batch_size=32, verbose=1)


nn_y_pred_prob = model.predict(X_test_scaled)
nn_y_pred = (nn_y_pred_prob > 0.45).astype(int)

nn_accuracy = accuracy_score(y_test, nn_y_pred)
nn_confusion_mat = confusion_matrix(y_test, nn_y_pred)
nn_precision = precision_score(y_test, nn_y_pred)
nn_recall = recall_score(y_test, nn_y_pred)
nn_f1 = f1_score(y_test, nn_y_pred)

print("Neural Network Model:")
print(f"Accuracy: {nn_accuracy:.2f}")
print("Confusion Matrix:")
nn_cm_df = pd.DataFrame(nn_confusion_mat, index=['Actual Down', 'Actual Up'], columns=['Predicted Down', 'Predicted Up'])
print(nn_cm_df)
print(f"Precision: {nn_precision:.2f}")
print(f"Recall: {nn_recall:.2f}")
print(f"F1-score: {nn_f1:.2f}")

sns.heatmap(nn_confusion_mat, annot=True)

"""## **Neural Networks as a Regression Model**"""

from sklearn.preprocessing import StandardScaler
from sklearn.metrics import mean_squared_error, mean_absolute_error
from tensorflow.keras.models import Sequential
from tensorflow.keras.layers import Dense
import numpy as np


scaler = StandardScaler()
X_train_scaled = scaler.fit_transform(X_train)
X_test_scaled = scaler.transform(X_test)


model = Sequential()
model.add(Dense(64, activation='relu', input_shape=(X_train_scaled.shape[1],)))
model.add(Dense(32, activation='relu'))
model.add(Dense(1))


model.compile(optimizer='adam', loss='mean_squared_error')
model.fit(X_train_scaled, y_train, epochs=50, batch_size=32, verbose=1)
nn_y_pred = model.predict(X_test_scaled)

mse = mean_squared_error(y_test, nn_y_pred)
rmse = np.sqrt(mse)
mae = mean_absolute_percentage_error(y_test, nn_y_pred)

print("Neural Network Regression Model:")
print(f"Mean Squared Error (MSE): {mse:.4f}")
print(f"Root Mean Squared Error (RMSE): {rmse:.4f}")


# print(f"Mean Absolute Percentage Error (MAPE): {mae:.4f}")
# import matplotlib.pyplot as plt

# Plot actual vs predicted values

plt.figure(figsize=(10, 6))
plt.plot(y_test, label='Actual', color='blue')
plt.plot(y_test.index,nn_y_pred.reshape(20), label='Predicted', color='red')
plt.title('Actual vs Predicted Values')
plt.xlabel('DATE')
plt.ylabel('UNRATE')
plt.legend()
plt.grid(True)
plt.show()

y_test

"""## **ROC Curve**"""

from sklearn.metrics import roc_curve, auc
from sklearn.metrics import confusion_matrix
import matplotlib.pyplot as plt
import numpy as np

nn_y_pred_prob = model.predict(X_test_scaled)

lr_y_pred_prob = lr_model.predict_proba(X_test_scaled)[:, 1]

dt_y_pred_prob = dt_model.predict_proba(X_test)[:, 1]

rfc_y_pred_prob = rfc_model.predict_proba(X_test)[:, 1]

nn_fpr, nn_tpr, nn_thresholds = roc_curve(y_test, nn_y_pred_prob)
lr_fpr, lr_tpr, lr_thresholds = roc_curve(y_test, lr_y_pred_prob)
dt_fpr, dt_tpr, dt_thresholds = roc_curve(y_test, dt_y_pred_prob)
rfc_fpr, rfc_tpr, rfc_thresholds = roc_curve(y_test, rfc_y_pred_prob)

nn_auc = auc(nn_fpr, nn_tpr)
lr_auc = auc(lr_fpr, lr_tpr)
dt_auc = auc(dt_fpr, dt_tpr)
rfc_auc=auc(rfc_fpr,rfc_tpr)

plt.figure(figsize=(8, 6))
plt.plot(nn_fpr, nn_tpr, label=f'Neural Network (AUC = {nn_auc:.2f})')
plt.plot(lr_fpr, lr_tpr, label=f'Logistic Regression (AUC = {lr_auc:.2f})')
plt.plot(dt_fpr, dt_tpr, label=f'Decision Tree (AUC = {dt_auc:.2f})')
plt.plot(rfc_fpr, rfc_tpr, label=f'Random Forest Classifier (AUC = {rfc_auc:.2f})')
plt.plot([0, 1], [0, 1], 'k--')
plt.xlabel('False Positive Rate')
plt.ylabel('True Positive Rate')
plt.title('ROC Curves')
plt.legend()
plt.show()

"""## **Cost Benefit Analysis and Profit Curve**"""

from sklearn.metrics import roc_curve, auc, confusion_matrix
import matplotlib.pyplot as plt
import numpy as np

nn_y_pred_prob = model.predict(X_test_scaled)

lr_y_pred_prob = lr_model.predict_proba(X_test_scaled)[:, 1]

dt_y_pred_prob = dt_model.predict_proba(X_test)[:, 1]

cost_benefit_matrix = np.array([[5, -2], [-10, 20]])


def calculate_profit(y_true, y_pred_prob, cost_benefit_matrix, thresholds):
    profits = []
    for threshold in thresholds:
        y_pred = (y_pred_prob >= threshold).astype(int)
        cm = confusion_matrix(y_true, y_pred)
        profit = np.sum(cm * cost_benefit_matrix)
        profits.append(profit)
    return np.array(profits)

nn_thresholds = np.linspace(0, 1, 100)
lr_thresholds = np.linspace(0, 1, 100)
dt_thresholds = np.linspace(0, 1, 100)

nn_profits = calculate_profit(y_test, nn_y_pred_prob, cost_benefit_matrix, nn_thresholds)
lr_profits = calculate_profit(y_test, lr_y_pred_prob, cost_benefit_matrix, lr_thresholds)
dt_profits = calculate_profit(y_test, dt_y_pred_prob, cost_benefit_matrix, dt_thresholds)

plt.figure(figsize=(8, 6))
plt.plot(nn_thresholds, nn_profits, label='Neural Network')
plt.plot(lr_thresholds, lr_profits, label='Logistic Regression')
plt.plot(dt_thresholds, dt_profits, label='Decision Tree')
plt.xlabel('Classification Threshold')
plt.ylabel('Profit')
plt.title('Profit Curves')
plt.legend()
plt.grid(True)
plt.show()

from sklearn.metrics import classification_report

target_names = ['Actual Decrease', 'Actual Increase']
print("DECISION TREE CLASSIFIER:")
print(classification_report(y_test, y_pred, target_names=target_names))
print("LOGISTIC CLASSIFIER:")
print(classification_report(y_test, lr_y_pred, target_names=target_names))
print("RANDOM FOREST CLASSIFIER:")
print(classification_report(y_test, rfc_y_pred_th, target_names=target_names))
print("NN CLASSIFIER:")
print(classification_report(y_test, nn_y_pred, target_names=target_names))

